{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import main libraries/packages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to ignore annoying IPython warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re # for some regexp magic ^-^\n",
    "\n",
    "# for the future use\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack # to get memory-efficient representation of matrices (sparse format)\n",
    "from textblob import TextBlob, Word # pip install textblob / conda install textblob\n",
    "\n",
    "# preprocessing / feature extraction / feature transformation\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA, SparsePCA\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.svm import LinearSVC, SVC # for the future blending/stacking, also - as baselines to beat\n",
    "from sklearn.linear_model import LogisticRegression # for the future blending/stacking, also - as baselines to beat\n",
    "from xgboost import XGBClassifier  # uncomment if you have it installed\n",
    "# how to install xgboost on windows - \n",
    "# https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows\n",
    "\n",
    "# model/feature aggregation in Pipelines\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# metrics/validation\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# model serialization/deserialization\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset import, base preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review count: 152610\n",
      "review count, no duplicates: 151871\n",
      "class balance: \n",
      " 1    89349\n",
      "0    62522\n",
      "Name: label, dtype: int64\n",
      "non-english reviews: 1716/151871\n",
      "review count, english only: 150155\n",
      "Wall time: 45.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# import review dataset for train (csv)\n",
    "\n",
    "# train datasets (RottenTomatoes + IMDB_small)\n",
    "train_df_names = ['reviews_rt_all.csv', 'imdb_small.csv']\n",
    "\n",
    "df = pd.concat((pd.read_csv(name, engine='c', sep='|', \n",
    "                 usecols=['label', 'text']) for name in train_df_names), ignore_index=True)\n",
    "print('review count: {}'.format(len(df)))\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "df.text = df.text.apply(lambda s: BeautifulSoup(s, 'lxml').text) #\n",
    "\n",
    "# drop duplicates, if any\n",
    "df.drop_duplicates(subset=['text'], inplace=True)\n",
    "print('review count, no duplicates: {}'.format(len(df)))\n",
    "\n",
    "# check for class balance\n",
    "print('class balance:', '\\n', df.label.value_counts())\n",
    "\n",
    "# check for language that differs from English (rough enough, we'll cut several \"almost english\" reviews)\n",
    "def is_english(s):\n",
    "    words = s.split()\n",
    "    non_english = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            w.encode('ascii')\n",
    "        except UnicodeEncodeError:\n",
    "            non_english += 1\n",
    "    return True if non_english*1.0/len(words) <= 0.05 else False\n",
    "\n",
    "df_nonenglish = df[~df['text'].apply(is_english)]\n",
    "print('non-english reviews: {}/{}'.format(len(df[~df['text'].apply(is_english)]), len(df)))\n",
    "\n",
    "# let's get rid of them\n",
    "df = df[df['text'].apply(is_english)]\n",
    "print('review count, english only: {}'.format(len(df)))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# don't use it, time-consuming (10-20 min on train/test preprocessing)\n",
    "\n",
    "# stop = set(stopwords.words('english'))\n",
    "\n",
    "# lemmatize texts (this can take a while...)\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "# we'll use TextBlob (nltk wrapper to get lemmatization/word sentiment etc.)\n",
    "lemmatize = lambda s: ' '.join([ Word(w[0]).lemmatize(get_wordnet_pos(w[1])) \n",
    "                                for w in TextBlob(s).pos_tags])\n",
    "# be patient, it takes up to 10-15 min\n",
    "#df['lemmatized'] = df.text.str.lower().str.replace('[\\d]', '').str.replace('[^\\w\\s]', ' ').apply(lemmatize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# for those who seek pain! :)\n",
    "# split by sentences ( 1 row with review(10 sentences long) -> 10 rows in dataframe, etc.)\n",
    "\n",
    "#s = df.apply(lambda x: pd.Series(x['sentences']),axis=1).stack().reset_index(level=1, drop=True)\n",
    "#s.name = 'sample'\n",
    "#df_extended = df.drop('sentences', axis=1).join(s)\n",
    "#print(df_extended.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 FEATURE ENGINEERING (CUSTOM FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_rate(s):\n",
    "    candidates = re.findall(r'(\\d{1,3}[\\\\|/]{1}\\d{1,2})', s)\n",
    "    rates = []\n",
    "    for c in candidates:\n",
    "        try:\n",
    "            rates.append(eval(c))\n",
    "        except SyntaxError:\n",
    "            pass\n",
    "        except ZeroDivisionError:\n",
    "            return 0\n",
    "    return np.median(rates)\n",
    "\n",
    "# regular expression to split review on sentences\n",
    "sentence_splitter = re.compile('(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\!|\\?|\\.)\\s')\n",
    "\n",
    "# lists of positive/negative smiles\n",
    "positive_smiles = set([\n",
    "\":‑)\",\":)\",\":-]\",\":]\",\":-3\",\":3\",\":->\",\":>\",\"8-)\",\"8)\",\":-}\",\":}\",\":o)\",\":c)\",\":^)\",\"=]\",\"=)\",\":‑D\",\":D\",\"8‑D\",\"8D\",\n",
    "\"x‑D\",\"xD\",\"X‑D\",\"XD\",\"=D\",\"=3\",\"B^D\",\":-))\",\";‑)\",\";)\",\"*-)\",\"*)\",\";‑]\",\";]\",\";^)\",\":‑,\",\";D\",\":‑P\",\":P\",\"X‑P\",\"XP\",\n",
    "\"x‑p\",\"xp\",\":‑p\",\":p\",\":‑Þ\",\":Þ\",\":‑þ\",\":þ\",\":‑b\",\":b\",\"d:\",\"=p\",\">:P\", \":'‑)\", \":')\",  \":-*\", \":*\", \":×\"\n",
    "])\n",
    "negative_smiles = set([\n",
    "\":‑(\",\":(\",\":‑c\",\":c\",\":‑<\",\":<\",\":‑[\",\":[\",\":-||\",\">:[\",\":{\",\":@\",\">:(\",\"D‑':\",\"D:<\",\"D:\",\"D8\",\"D;\",\"D=\",\"DX\",\":‑/\",\n",
    "\":/\",\":‑.\",'>:\\\\', \">:/\", \":\\\\\", \"=/\" ,\"=\\\\\", \":L\", \"=L\",\":S\",\":‑|\",\":|\",\"|‑O\",\"<:‑|\"\n",
    "])\n",
    "\n",
    "# pattern to catch SUCH WORDS and ignore SuCH :)\n",
    "uppercase_pattern = re.compile(r'(\\b[0-9]*[A-Z]+[0-9]*[A-Z]{1,}[0-9]*\\b)')\n",
    "\n",
    "# contrast conjugations\n",
    "contrast_conj = set([\n",
    "'alternatively','anyway','but','by contrast','differ from','elsewhere','even so','however','in contrast','in fact',\n",
    "'in other respects','in spite of','in that respect','instead','nevertheless','on the contrary','on the other hand',\n",
    "'rather','though','whereas','yet'])\n",
    "\n",
    "# to get review \"purity\" ~ same sentiment over review (~1) or not (~0)\n",
    "def purity(sentences):\n",
    "    polarities = np.array([TextBlob(x).sentiment.polarity for x in sentences])\n",
    "    return polarities.sum() / np.abs(polarities).sum()\n",
    "\n",
    "# feature engineering ^-^\n",
    "def get_custom_features(text):\n",
    "    # assume text = pd.Series with review text\n",
    "    print('extracting custom features...')\n",
    "    tdf = pd.DataFrame()\n",
    "    tdf['text'] = text \n",
    "    tdf['sentences'] = tdf.text.apply(lambda s: re.split(sentence_splitter, s)) # split to sentences\n",
    "    \n",
    "    tdf['sentence_cnt'] = tdf['sentences'].apply(len) # feature 1 - (sentence count)\n",
    "    tdf['exclamation_cnt'] = tdf.text.str.count('\\!') # feature 2 - (exclamation mark count)\n",
    "    tdf['question_cnt'] = tdf.text.str.count('\\?') # feature 3 - (question mark count)\n",
    "    \n",
    "    # feature 4 - totally uppercase words (like HOLY JESUS!)\n",
    "    tdf['upper_word_cnt'] = tdf.text.apply(lambda s: len(re.findall(uppercase_pattern, s)))\n",
    "    \n",
    "    # try to extract rating :) like \"great film. 9/10\" will yield 0.9\n",
    "    tdf['rating'] = tdf['text'].apply(get_rate).fillna(-1) # feature 5 - rating (if found in review)\n",
    "\n",
    "    # try to extract smiles and count positive/negative smiles per review (features 6,7)\n",
    "    tdf['positive_smiles'] = tdf.text.apply(lambda s: len([x for x in s.split() if x in positive_smiles]))\n",
    "    tdf['negative_smiles'] = tdf.text.apply(lambda s: len([x for x in s.split() if x in negative_smiles]))\n",
    "    \n",
    "    # not so informative, but still\n",
    "    tdf['contrast_conj_cnt'] = tdf.text.apply(lambda s: len([c for c in contrast_conj if c in s]))\n",
    "    \n",
    "    # feature 8 (polarity of 1st sentence)\n",
    "    tdf['polarity_1st_sent'] = tdf.sentences.apply(lambda s: TextBlob(s[0]).sentiment.polarity)\n",
    "    # feature 9 (subjectivity of 1st sentence)\n",
    "    tdf['subjectivity_1st_sent'] = tdf.sentences.apply(lambda s: TextBlob(s[0]).sentiment.subjectivity)\n",
    "    # feature 10 (polarity of last sentence)\n",
    "    tdf['polarity_last_sent'] = tdf.sentences.apply(lambda s: TextBlob(s[-1]).sentiment.polarity)\n",
    "    # feature 11 (subjectivity of last sentence)\n",
    "    tdf['subjectivity_last_sent'] = tdf.sentences.apply(lambda s: TextBlob(s[-1]).sentiment.subjectivity)\n",
    "    # feature 12 (subjectivity of review itself)\n",
    "    tdf['polarity'] = tdf.text.apply(lambda s: TextBlob(s[-1]).sentiment.polarity)\n",
    "    # feature 13 (\"purity\" of review, |sum(sentence polarity) / sum(|sentence polarity|))|, ~ 1 is better, ~ 0 -> mixed\n",
    "    tdf['purity'] = tdf.sentences.apply(purity)\n",
    "    tdf['purity'].fillna(0, inplace=True)\n",
    "    \n",
    "    return csr_matrix(tdf[tdf.columns[2:]].values) # to get sparse format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2 FEATURE EXTRACTION (bag-of-words, tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a list of data extractors/transformers (format = [('ft1_name', ft1_object), ('ft2_name', ft2_object), ...])\n",
    "\n",
    "extraction_list = []\n",
    "\n",
    "# 1. custom features\n",
    "extraction_list.append(['custom_features', \n",
    "                             FunctionTransformer(func=get_custom_features,\n",
    "                                                 validate=False,\n",
    "                                                 accept_sparse=True\n",
    "                                                )\n",
    "                            ])\n",
    "# 2. simple bag-of-words (tf-idf)\n",
    "extraction_list.append(['tfidf', \n",
    "                             TfidfVectorizer(decode_error='ignore',\n",
    "                                             max_df=0.4, \n",
    "                                             min_df=3,\n",
    "                                             ngram_range=(1, 3),\n",
    "                                             max_features=20000,\n",
    "                                            #stop_words='english'\n",
    "                                            )\n",
    "                            ])\n",
    "\n",
    "extractor = FeatureUnion(extraction_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = XGBClassifier(max_depth=7, \n",
    "                    n_estimators=700, \n",
    "                    min_child_weight=3, \n",
    "                    learning_rate=0.2, \n",
    "                    gamma=0, \n",
    "                    reg_lambda=1, \n",
    "                    reg_alpha=0, \n",
    "                    subsample=1, \n",
    "                    nthread=7 # be careful, CPU overload is high, up to 96%\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create pipeline, combining steps together                                                                                                                       \n",
    "\n",
    "model = Pipeline(\n",
    "    [\n",
    "        ('feature_extraction', extractor),\n",
    "        ('clf', clf)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting custom features...\n",
      "finally fitted :)\n",
      "extracting custom features...\n",
      "Accuracy on validation: 0.8118610768872165\n",
      "Wall time: 11min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                                    df.text, \n",
    "                                                    df.label, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=df.label\n",
    "                                                   )\n",
    "\n",
    "\n",
    "\n",
    "# fit model\n",
    "model.fit(X_train, y_train)\n",
    "print('finally fitted :)')\n",
    "\n",
    "#check results on validation\n",
    "print('Accuracy on validation: {}'.format(accuracy_score(model.predict(X_test), y_test)))\n",
    "\n",
    "'''\n",
    "# params for grid search\n",
    "params = {\n",
    "    'feature_extraction__tfidf__max_df': [0.3, 0.4, 0.7],\n",
    "}\n",
    "\n",
    "# grid search\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "gs = GridSearchCV(model, param_grid=params, scoring='accuracy', verbose=2, error_score=1, cv=cv)\n",
    "gs.fit(X_train, y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model...\n",
      "model saved in file H:\\EDUCATION\\KAGGLE\\UKRAINIAN DATA SCIENCE CLUB\\udc-repository\\XGB-model.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "filename = 'XGB-model.pkl'\n",
    "try:\n",
    "    with open(filename, 'wb') as f:\n",
    "        print('saving model...')\n",
    "        dill.dump(model, f)\n",
    "        print('model saved in file {}'.format(os.getcwd() + os.sep + filename))\n",
    "except:\n",
    "    print('Errors in model dump...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting custom features...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small example of usage\n",
    "text = pd.Series(['nice film!', \n",
    "                  'it is total crap. 2/10', \n",
    "                  'overall, 2/10, however, plot was rather good', \n",
    "                  'ok, 7/10'])\n",
    "model.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
